#include <stdlib.h>
#include <stdio.h>
#include <string.h>
#include <signal.h>
#include <time.h>
#include <math.h>

#include "chart.h"
#include "rule.h"
#include "lexicon.h"
#include "morpho.h"
#include "mrs.h"
#include "unpack.h"
#include "token.h"
#include "lattice-mapping.h"
#include "lexical-parse.h"
#include "timer.h"
#include "profiler.h"
#include "timeout.h"

#include "supertag.h"
#include "ubertag.h"

double ubertagging_threshold = 0.0001;
int give_up_threshold = 70;

void itsdb_dump_tokens(char *name, char *countname, struct lattice *tokens);
int itsdb_report_parse(struct lattice *tokens, struct lattice *lexical_chart, int status, int tforest);

struct lattice *last_token_chart;

extern int total_edges, real_edges, passive_edges, used_edges, pused_edges, trace, used_total, pused_total;
extern int question, exhaustive, lui_mode, inhibit_pack, inhibit_results, do_itsdb, do_itsdb_stdout, best_only;
extern float timer();

#ifdef LEVEL_TIMER
float level_times[100];
#endif
int level = 0;
int do_forest = 0;
extern int debug_level;
int yy_mode = 0;
int do_improve_characterization = 0;

int g_profiling = 0;

struct profiler *start_sign_profiler(struct edge *e)
{
	static struct profiler **active_rule_profilers = NULL;
	static struct profiler **passive_rule_profilers = NULL;
	static struct profiler *passive_lex_profiler = NULL;
	chart_parsing_profiler->sortable = 10;
	if (!active_rule_profilers)
	{
		active_rule_profilers = calloc(sizeof(struct profiler *), nrules);
		passive_rule_profilers = calloc(sizeof(struct profiler *), nrules);
	}
	if (e->have == e->need)
	{
		if (e->have == 0)
		{
			start_and_alloc_profiler(&passive_lex_profiler, "lexeme", chart_parsing_profiler, NULL);
			return passive_lex_profiler;
		}
		struct rule *r = e->rule;
		char name[1024];
		sprintf(name, "passive: %s", r->name);
		start_and_alloc_profiler(&passive_rule_profilers[r->rnum], name, chart_parsing_profiler, NULL);
		return passive_rule_profilers[r->rnum];
	}
	else
	{
		struct rule *r = e->rule;
		char name[1024];
		sprintf(name, "active: %s", r->name);
		start_and_alloc_profiler(&active_rule_profilers[r->rnum], name, chart_parsing_profiler, NULL);
		return active_rule_profilers[r->rnum];
	}
}

static int parse_process_edge(struct edge *e)
{
	int i, j, remain, pack = 0, from = e->from, to = e->to, have = e->have, nlist = 0;
	struct chart_cell *c;
	static struct edge **list = 0;
	static int alist = 0;

	// XXX experimental chart-pruning-like idea
	if (e->have == e->need && e->rule && !predict_1_rule_use(e->rule->name, e->from, e->to))
	{
		// printf("predicted we don't want #%d %s [%d - %d]\n",
		// e->id, e->rule->name, e->from, e->to);
		return 0;
	}

	// block() prevents in-chart edges built on frozen edges from playing further.
	// edges that were on the agenda at that time need to be block'd here.
	// also block cases where the daughter is a generalized edge all of whose pack-ees have been vaniquished (as block() does)
	// question: should this test be moved to pack_edge()?  not unless we call pack_edge() on both active and passives, maybe?
	if (e->daughters)
		for (i = 0; i < e->have; i++)
		{
			if (e->daughters[i]->frozen == 1)
			{
				// printf("NOT giving edge #%d a chance to try to pack; daughter[%d] = edge #%d was frozen\n", e->id, i, e->daughters[i]->id);
				return 0;
			}
			if (!e->daughters[i]->daughters && !e->daughters[i]->lex && !e->daughters[i]->npack)
				return 0;
		}
	int jj;
	// reduce list of prepacked edges by the same technique
	for (j = jj = 0; j < e->npack; j++)
	{
		struct edge *p = e->pack[j];
		assert(p->daughters || p->lex); // can't have a prepacked generalization edge; should have been flattened if so.
		for (i = 0; i < p->have; i++)
		{
			if (p->daughters[i]->frozen == 1)
			{
				// printf("NOT giving edge #%d a chance to try to pack; daughter[%d] = edge #%d was frozen\n", e->id, i, e->daughters[i]->id);
				continue;
			}
			if (!p->daughters[i]->daughters && !p->daughters[i]->lex && !p->daughters[i]->npack)
				continue;
		}
		e->pack[jj++] = p;
	}
	e->npack = jj;

#ifdef LEVEL_TIMER
	if (!e->lex && (e->to - e->from != level))
	{
		if (e->to > e->from)
			level_times[e->to - e->from - 1] = timer();
		level = e->to - e->from;
	}
#endif

	if (!inhibit_pack && e->have == e->need) // why not pack actives too?
	{
		static int packtimer = -1;
		if (packtimer == -1)
			packtimer = new_timer("packing");
		IFCLOCK(start_timer(packtimer););
		if (g_profiling)
			start_and_alloc_profiler(&packing_profiler, "packing", chart_parsing_profiler, NULL);
		pack = pack_edge(e);
		if (g_profiling)
			stop_profiler(packing_profiler, 1);
		IFCLOCK(stop_timer(packtimer, 1););
		if (pack == 1)
			return 0; // big savings! woohoo!
	}
	if (pack == 0)
		add_edge_to_chart(e);

	struct profiler *prof = NULL;
	if (g_profiling)
		prof = start_sign_profiler(e);

	if (!alist)
	{
		list = calloc(sizeof(struct edge *), 4);
		alist = 4;
	}
#define LADD(e)                                                  \
	do                                                           \
	{                                                            \
		if (nlist + 1 > alist)                                   \
		{                                                        \
			alist *= 2;                                          \
			list = realloc(list, sizeof(struct edge *) * alist); \
		}                                                        \
		list[nlist++] = (e);                                     \
	} while (0)

	// try to combine with edges on either side
	if (e->have == e->need)
	{
		//		char	fil[nrules*2];
		//		if(constraint_rule_filter(e->lex, e->id, e->dg, fil) != 0)
		//			return 0;

		c = cells + from;
		// passive edge
		for (i = 0; i <= from; i++)
		{
			struct edge **E = c[i * chart_size].al_edges, **F = E + c[i * chart_size].al_nedges;
			for (; E < F; E++)
				if (to + ((*E)->need - (*E)->have - 1) < chart_size)
					//				if(!fil[2*(*E)->rule->rnum + 0] || !fil[2*(*E)->rule->rnum + 1])
					LADD(*E);
		}
		c = cells + to * chart_size;
		for (i = to; i < chart_size; i++)
		{
			struct edge **E = c[i].ar_edges, **F = E + c[i].ar_nedges;
			for (; E < F; E++)
				if (from - ((*E)->need - (*E)->have - 1) >= 0)
					//				if(!fil[2*(*E)->rule->rnum + 0] || !fil[2*(*E)->rule->rnum + 1])
					LADD(*E);
		}
	}
	else
	{
		// active edge
		remain = e->need - e->have;
		if (e->rtl)
		{
			for (i = 0, c = cells + from; i <= from; i++)
			{
				struct edge **E = c[i * chart_size].p_edges, **F = E + c[i * chart_size].p_nedges;
				for (; E < F; E++)
					if ((*E)->from - (remain - 1) >= 0)
						LADD(*E);
			}
		}
		else
		{
			for (i = to, c = cells + to * chart_size; i < chart_size; i++)
			{
				struct edge **E = c[i].p_edges, **F = E + c[i].p_nedges;
				for (; E < F; E++)
					if ((*E)->to + (remain - 1) < chart_size)
						LADD(*E);
			}
		}
	}
	// printf(" edge #%d found %d potential neighbors\n", e->id, nlist);

	if (!nlist)
	{
		if (prof)
			stop_profiler(prof, 1);
		return 0;
	}

	if (e->have == e->need)
	{
		__builtin_prefetch(e->rule);
		for (i = j = 0; i < nlist; i++)
		{
			if (i + 2 < nlist)
				__builtin_prefetch(list[i + 2]);
			if (i + 1 < nlist)
				__builtin_prefetch(list[i + 1]->rule);
			if (filter(list[i], e))
				list[j++] = list[i];
		}
	}
	else
	{
		__builtin_prefetch(e->rule);
		for (i = j = 0; i < nlist; i++)
		{
			if (i + 2 < nlist)
				__builtin_prefetch(list[i + 2]);
			if (i + 1 < nlist)
				__builtin_prefetch(list[i + 1]->rule);
			if (filter(e, list[i]))
				list[j++] = list[i];
		}
	}
	nlist = j;

	if (!nlist)
	{
		if (prof)
			stop_profiler(prof, 1);
		return 0;
	}

	/*
	int	cmp(const void	*a, const void	*b)
	{
		struct dg	*da = (*(const struct edge**)a)->dg;
		struct dg	*db = (*(const struct edge**)b)->dg;
		if(da && db)return db->gen - da->gen;
		else if(da)return -1;
		else if(db)return 1;
		else return 0;
	}
	qsort(list, nlist, sizeof(void*), cmp);
	*/

	if (e->have == e->need)
	{
		for (i = 0; i < nlist; i++)
			if (unify_and_build(list[i], e) == -1)
				return -1;
	}
	else
	{
		for (i = 0; i < nlist; i++)
			if (unify_and_build(e, list[i]) == -1)
				return -1;
	}
	if (prof)
		stop_profiler(prof, 1);
	return 0;
}

int fits(struct rule *r, int d, struct edge *e)
{
	if (e->rule)
		return check_rf(r, e->rule, d);
	if (d == r->ndaughters - 1)
		return check_trl(r, e->lex->lextype);
	return 1;
}

int quickcheck_rule(int rnum, int d, struct qc *q)
{
	static struct qc **rqc = 0;
	if (!rqc)
	{
		int i;
		rqc = malloc(sizeof(struct qc *) * nrules * 2);
		for (i = 0; i < nrules; i++)
		{
			rqc[2 * i + 0] = extract_qc_arg(rules[i]->packdg, 0);
			if (rules[i]->ndaughters > 1)
				rqc[2 * i + 1] = extract_qc_arg(rules[i]->packdg, 1);
		}
	}
	return quickcheck(rqc[2 * rnum + d], q);
}

void halftime_analysis()
{
	struct chart_cell *right = cells + (chart_size - 1) + (chart_size - 2) * chart_size;
	char parent[nrules], uleftsib[nrules], leftsib[nrules];
	int i, j;

	bzero(parent, sizeof(parent));
	bzero(uleftsib, sizeof(parent));
	bzero(leftsib, sizeof(parent));
	// printf("rightmost passive edges: %d\n", right->p_nedges);
	for (i = 0; i < right->p_nedges; i++)
	{
		struct edge *e = right->p_edges[i];
		for (j = 0; j < nrules; j++)
			if (!rules[j]->rtl && rules[j]->ndaughters > 1)
				if (fits(rules[j], rules[j]->ndaughters - 1, e) &&
					!quickcheck_rule(j, rules[j]->ndaughters - 1, e->qc))
					parent[j] = 1;
	}
	// printf("rightmost active RL edges: %d\n", right->ar_nedges);
	for (i = 0; i < right->ar_nedges; i++)
	{
		struct edge *e = right->ar_edges[i];
		if (!e->have)
			continue;
		parent[e->rule->rnum] = 1;
	}

	for (i = 0; i < nrules; i++)
		if (parent[i])
			printf("	right daughter of %s\n", rules[i]->name);

	// mark all unaries which fit the left slot of the parent, recursively

	int again;
	do
	{
		again = 0;
		for (i = 0; i < nrules; i++)
			if (parent[j])
				for (j = 0; j < nrules; j++)
					if (rules[j]->ndaughters == 1 && check_rf(rules[i], rules[j], 0) && !parent[j])
						parent[j] = 1, again = 1;
	} while (again);

	// find all binaries that fit into the parents or the unaries
	for (i = 0; i < nrules; i++)
		if (parent[i])
			for (j = 0; j < nrules; j++)
				if (check_rf(rules[i], rules[j], 0))
					leftsib[j] = 1;

	for (i = 0; i < nrules; i++)
		if (leftsib[i])
			printf("	left sibling could be %s\n", rules[i]->name);
}

// int		wcap[1024];
char current_sentence[10240] = "(not initialized)";
int didsent;
int show_probability = 0;

int prepare_parse_chart(struct lattice *token_chart, struct lattice *lexical_chart)
{
	sort_lattice(lexical_chart);

	int nwords = lexical_chart->lattice_vertex_id_gen - 1;

	// setup the chart
	clear_chart(nwords + 1);

	// put lexical edges into agenda
	int i;
	// note: edges need to be process_edge()'d into the chart in the same order they were discovered in lexical parsing,
	// which should be the same order they appear in the lexical_chart->edges list.
	// if we dont do it in the right order, we can violate some assumptions of the retroactive packing system
	// and end up with spurious readings.
	for (i = 0; i < lexical_chart->nedges; i++)
	{
		struct lattice_edge *le = lexical_chart->edges[i];
		assert(le->edge != NULL);
		le->edge->from = le->from->id;
		le->edge->to = le->to->id;
		add_agenda(le->edge);
	}

	// optionally add edges from a robust shallow parse tree
	if (!yy_mode)
		dublin_robustness(lexical_chart, current_sentence);

	// put syntax rules into agenda
	// if(rtried)free(rtried);
	// rtried = calloc(sizeof(int),nrules);
	agenda_rules(nwords);

	return nwords;
}

void (*parse_callback)(struct hypothesis *hyp, struct mrs *mrs) = NULL;

// show analyses
int parse_show_result(struct hypothesis *hyp, struct mrs *mrs, double prob)
{
	struct dg *root = hyp->dg, *sem;
	extern char *mrs_end;
	extern int print_trees;
	if (trace > 0)
		printf("edge #%d has a sentential unpacking\n", hyp->edge->id);
	if (trace > 1)
	{
		print_dg(hyp->dg);
		printf("\n");
	}
	if (!inhibit_results)
	{
		char *sentence = current_sentence;
		if (sentence && !question && !didsent)
		{
			printf("SENT: %s\n", sentence);
			didsent = 1;
		}
		if (show_probability)
			printf("probability = %f\n", prob);
		if (print_trees)
		{
			mrs_end = " ; ";
			print_mrs(mrs);
			extern int report_labels;
			char *dstr = report_labels ? hypothesis_to_labelled_tree_string(hyp) : hypothesis_to_dstring(hyp);
			fflush(stdout);
			puts(dstr);
			fflush(stdout);
			mrs_end = "\n";
			/*extern int saved_termerr;
			write(saved_termerr, dstr, strlen(dstr));
			write(saved_termerr, mrs_end, 1);*/
			// print_hypothesis(hyp, printf, 0);
			// printf((mrs_end = "\n"));
			free(dstr);
		}
		else
			print_mrs(mrs);
	}
	if (parse_callback)
		parse_callback(hyp, mrs);
	free_mrs(mrs);
	// printf("\n");
	// printf("dstring = `%s'\n", slab_escape(hypothesis_to_dstring(hyp)));
	return 1;
}

static int chart_setup_timer = -1;

// OZ: Adding sent_num for the oracle supertagger setup where the supertagger simply looks up oracle tags for each sentence from a file
int parse(int nwords, char **words, int *cfrom, int *cto, char *sentence, int sent_num)
{
	clock_t start = clock();

	if (chart_setup_timer == -1)
		chart_setup_timer = new_timer("chart setup");
	start_timer(chart_setup_timer);

	if (g_profiling)
		start_and_alloc_profiler(&token_mapping_profiler, "token mapping", parse_profiler, preprocess_profiler);

	// setup a token lattice and do token mapping
	struct lattice *token_chart = build_token_chart(nwords, words, cfrom, cto);
	if (!token_chart)
	{
		stop_timer(chart_setup_timer, 1);
		return -1;
	}
	return parse_with_token_chart(token_chart, start, sent_num, nwords);
}

int parse_with_token_chart(struct lattice *token_chart, clock_t start, int sent_num, int sen_len)
{
	int i, count, cforest;
	int num_parses = 0, num_entries;
	float t_setup, t_forest, t_unpack;
	struct dg *root;
	struct dg *sem;
	struct edge *edge;

	if (do_itsdb)
		itsdb_dump_tokens(":p-input", ":ninputs", token_chart);
	apply_token_mapping(token_chart);
	sort_lattice(token_chart);
	if (!yy_mode && do_improve_characterization)
		improve_token_lattice_characterization(token_chart);

	if (token_chart->nvertices > give_up_threshold)
	{
		fprintf(stderr, "NOTE: giving up, too many words (%d+)\n", give_up_threshold);
		itsdb_error("too many words");
		if (do_itsdb)
			itsdb_report_parse(token_chart, NULL, -2, 0);
		stop_timer(chart_setup_timer, 1);
		return -2;
	}

	if (trace)
		print_token_chart(token_chart);

	if (trace)
		printf("finished token mapping\n");

	if (g_profiling)
		start_and_alloc_profiler(&lexical_lookup_profiler, "lexical lookup", parse_profiler, token_mapping_profiler);
	// do lexical lookup
	struct lattice *lexical_chart = lexical_lookup_into_chart(token_chart);
	// printf("Built a lexical chart\n");
	// print_lexical_chart(lexical_chart);

	if (!lexical_chart)
	{
		fprintf(stderr, "NOTE: failed to build lexical chart\n");
		stop_timer(chart_setup_timer, 1);
		return -1;
	}
	if (trace)
		printf("finished lexical lookup\n");

	if (g_profiling)
		start_and_alloc_profiler(&lexical_parsing_profiler, "lexical parsing", parse_profiler, lexical_lookup_profiler);

	extern int chart_size;
	chart_size = 0; // set to a predictable value rather than leaving it whatever the last sentence had; used in deciding whether an edge is spanning or not, as early as lexical parsing, for deciding whether the rule_root_utc[] is applicable.

	extern int reduce_chart_before_lexical_parsing;

	// for(i=0;i<lexical_chart->nedges;i++)
	// {
	// 	if (lexical_chart->edges[i]->token->cfrom > 1000)
	// 		printf("Ouch\n");
	// }

	if (!reduce_chart_before_lexical_parsing)
	{
		// do lexical parsing
		int rv = lexical_parse_lattice(lexical_chart);
		// printf("Reduced lexical chart\n");
		// print_lexical_chart(lexical_chart);
		if (rv == -1)
		{
			itsdb_error("ran out of RAM in lexical parsing");
			return -1; // ran out of memory in lexical parsing!
		}
	}

	if (reduce_lexical_lattice(lexical_chart, token_chart))
	{
		fprintf(stderr, "NOTE: post reduction gap\n");
		stop_timer(chart_setup_timer, 1);
		if (do_itsdb)
			itsdb_report_parse(token_chart, lexical_chart, -2, 0);
		return -2;
	}

	if (reduce_chart_before_lexical_parsing)
	{
		lexical_parse_lattice(lexical_chart);
		// printf("Lexical chart after calling lexical_parse_lattice\n");
		// print_lexical_chart(lexical_chart);
	}

	if (trace)
		print_lexical_chart(lexical_chart);
	if (trace)
		printf("finished lexical parsing\n");

	if (g_profiling)
		start_and_alloc_profiler(&lexical_filtering_profiler, "lexical filtering", parse_profiler, lexical_parsing_profiler);

	// do lexical filtering
	apply_lexical_filtering(lexical_chart);
	if (trace)
		printf("finished lexical filtering\n");
	// printf("Lexical chart after lexical filtering\n");
	// print_lexical_chart(lexical_chart);

	extern struct ubertagger *the_ubertagger;
	extern int enable_ubertagging;
	extern struct supertagger *the_supertagger;
	extern int enable_supertagging;
	extern char *supertags_path;
	if (the_supertagger && enable_supertagging)
	{
		//printf("Lexical chart before supertagging:\n");
		//print_lexical_chart(lexical_chart);
		supertag_lattice(the_supertagger, lexical_chart, sent_num);
		// printf("Lexical chart after supertagging:\n");
		// print_lexical_chart(lexical_chart);
	}
	if (g_profiling)
		start_and_alloc_profiler(&ubertagging_profiler, "übertagging", parse_profiler, lexical_filtering_profiler);
	if (the_ubertagger && enable_ubertagging)
	{
		// printf("Lexical chart before ubertagging:\n");
		// print_lexical_chart(lexical_chart);
		ubertag_lattice(the_ubertagger, lexical_chart, log(ubertagging_threshold));
		// printf("Lexical chart after ubertagging:\n");
		// print_lexical_chart(lexical_chart);
	}

	if (g_profiling)
		start_and_alloc_profiler(&chart_parsing_profiler, "chart parsing", parse_profiler, ubertagging_profiler);

	// XXX far-fetched experiment...
	// first_pass_parse(lexical_chart);
	// return 1;

	// lexemes have no packing restrictor applied to them thus far.
	// make sure all copy()'s from here on out (until unpacking time) are restricting.
	if (!inhibit_pack)
		enable_packing(1);
	// setup the main parse chart
	int nwords = prepare_parse_chart(token_chart, lexical_chart);
	if (nwords < 0)
	{
		fprintf(stderr, "NOTE: negative lexical chart length\n");
		itsdb_error("negative lexical chart length");
		if (do_itsdb)
			itsdb_report_parse(token_chart, lexical_chart, -2, 0);
		stop_timer(chart_setup_timer, 1);
		enable_packing(0);
		return -2;
	}
	assert(nwords >= 0);
	if (!nwords)
	{
		fprintf(stderr, "NOTE: nothing to parse\n");
		itsdb_error("nothing to parse");
		if (do_itsdb)
			itsdb_report_parse(token_chart, lexical_chart, -2, 0);
		stop_timer(chart_setup_timer, 1);
		enable_packing(0);
		return -2;
	}

	// XXX experimental chart-pruning-like idea
	predict_rule_uses(nwords, lexical_chart);

	cancel_task = 0;
	did_timeout = 0;
	signal(SIGALRM, alarm_handler);

	if (timeout_seconds > 0)
		alarm(timeout_seconds);

	stop_timer(chart_setup_timer, 1);
	static int chart_parsing_timer = -1;
	if (chart_parsing_timer == -1)
		chart_parsing_timer = new_timer("chart parsing");
	start_timer(chart_parsing_timer);

	t_setup = timer();

	// do the actual parsing
	int half = nwords / 2;
	double t_half;
	while (!cancel_task && (edge = next_agenda(0)))
	{
		if (edge->to - edge->from >= half)
		{
			half = nwords + 2;
			t_half = timer();
			// halftime_analysis();
		}
		if (parse_process_edge(edge))
			break;
	}

	compact_generalization_edges();

	enable_packing(0);

	stop_timer(chart_parsing_timer, 1);

	if (g_profiling)
		start_and_alloc_profiler(&unpacking_profiler, "unpacking", parse_profiler, chart_parsing_profiler);

	t_forest = t_half + timer();
	cforest = clock() - start;

	if (do_forest)
	{
		char *sentence = current_sentence;
		fflush(stdout);
		if (sentence)
			fprintf(stderr, "SENT: %s\n", sentence);
		int found_root = output_forest(&cells[chart_size - 1], token_chart);
		fflush(stdout);
		fprintf(stderr, "NOTE: %d readings [forest], ", found_root);
		print_slab_stats();
		fprintf(stderr, "\n");
		num_parses = 1;
	}
	else if (!do_itsdb && !lui_mode)
	{
		didsent = 0;
		if (chart_size > 0)
			num_parses = iterate_cell_root_hypotheses(&cells[chart_size - 1], parse_show_result, best_only);
		else
			num_parses = 0;

		fprintf(stderr, "NOTE: %d readings, added %d / %d edges to chart (%d fully instantiated, %d actives used, %d passives used)\t",
				num_parses, total_edges, real_edges, passive_edges, used_edges, pused_edges);
		print_slab_stats();

		// rule_profiler();	// XXX comment me out normally
	}
	else if (do_itsdb)
	{
		num_parses = itsdb_report_parse(token_chart, lexical_chart, 0, (int)((long long)cforest * 1000 / CLOCKS_PER_SEC));
	}
	else if (lui_mode)
	{
		num_parses = output_lui_trees(cells + chart_size - 1, current_sentence, best_only, 0);
		if (!num_parses)
		{
		} // output_lui_chart(words);
		last_token_chart = token_chart;
	}
	alarm(0);

	//	extern int	output_edge_vectors;
	//	if(output_edge_vectors)
	//		do_output_edge_vectors();

	t_unpack = timer();

	used_total += used_edges;
	pused_total += pused_edges;

	if (0)
	{
		static FILE *f = 0;
		// extern long long ndecomposed;
		// static long long ond = 0;
		if (!f)
			f = fopen("/tmp/times", "w");
		fprintf(f, "%d	%d	%f	%f	%f	%s\n", nwords, num_parses, t_setup, t_forest, t_unpack, current_sentence);
		// ond = ndecomposed;
		fflush(f);
	}

	return num_parses;
}

void preprocess_line(char *string, int *Nwords, char ***Words, int **CFrom, int **CTo);

int parse_line1(char *line, int sent_num)
{
	char *copy = NULL;
	int nparse = 0;
	int nwords = 0, i, j;
	int *cfrom = NULL, *cto = NULL;
	char **words = NULL, *p, *q;
	extern int preprocess_only;

	timer();
	extern int tsdb_notes;
	if (tsdb_notes)
		tsdb_parse_note_start();
	if (g_profiling)
	{
		if (parse_profiler)
			stop_profilers_recursively(parse_profiler);
		start_and_alloc_profiler(&parse_profiler, "parsing", NULL, NULL);
	}

	last_token_chart = NULL;

	// a starting place for main-chart edge IDs
	reset_edges();

	if (yy_mode)
	{
		strcpy(current_sentence, "(yy mode)");
		nparse = parse_yy(line);
		goto finished;
	}

	strcpy(current_sentence, "(preparing)");
	while (line[0] && line[strlen(line) - 1] == '\n')
		line[strlen(line) - 1] = 0;
	int check_mbs(char *mbs);
	if (!*line || check_mbs(line) == -1)
	{
		if (!inhibit_results && !do_itsdb_stdout)
		{
			printf("SKIP: %s\n", line);
			fflush(stdout);
		}
		if (do_itsdb)
			itsdb_report_parse(NULL, NULL, -1, 0);
		return -1;
	}
	if (!strncmp(line, "<DOC", 4))
	{
		printf("%s\n", line);
		return -1;
	}

	copy = strdup(line);

	if (g_profiling)
		start_and_alloc_profiler(&preprocess_profiler, "REPP", parse_profiler, NULL);
	preprocess_line(copy, &nwords, &words, &cfrom, &cto);
	if (preprocess_only)
	{
		if (!inhibit_results)
		{
			for (i = 0; i < nwords; i++)
			{
				printf("%s%s", i ? " " : "", words[i]);
				if (debug_level)
					printf("<%d:%d>", cfrom[i], cto[i]);
			}
			printf("\n");
		}
		goto finished;
	}

	if (nwords > 200)
	{
		fprintf(stderr, "NOTE: 0 readings, too many words (200+)\n");
		itsdb_error("too many words (200+ before token mapping)");
		if (!inhibit_results && !do_itsdb_stdout)
		{
			printf("SKIP: %s\n", line);
			fflush(stdout);
		}
		goto finished;
	}

	strncpy(current_sentence, line, 10239);
	current_sentence[10239] = 0;
	level = 0;
	timer();
	nparse = parse(nwords, words, cfrom, cto, current_sentence, sent_num);
	if (nparse <= 0 && !inhibit_results && !do_itsdb_stdout)
	{
		printf("SKIP: %s\n", line);
		fflush(stdout);
	}

#ifdef LEVEL_TIMER
	level_times[nwords] = timer();
	for (i = 1; i <= nwords; i++)
		printf(" level %d: %f\n", i, level_times[i]);
#endif

	if (nparse < -1)
		fprintf(stderr, "NOTE: ignoring `%s'\n", current_sentence);
	if (do_itsdb && nparse == -1)
		itsdb_report_parse(NULL, NULL, -1, 0);

finished:
	if (tsdb_notes)
		tsdb_parse_note_end();
	if (g_profiling)
		stop_profilers_recursively(parse_profiler);
	clear_mrs();
	for (i = 0; i < nwords; i++)
		free(words[i]);
	if (words)
		free(words);
	if (cfrom)
		free(cfrom);
	if (cto)
		free(cto);
	if (copy)
		free(copy);
	return nparse;
}

int parse_line(char *line, int sent_num)
{
	int res = parse_line1(line, sent_num);
	if (!inhibit_results)
		printf("\n\n");
	fflush(stdout);
	return res;
}

show_last_parse_chart()
{
	// ... the slab has been clear_slab()'d, but
	// we likely still own the memory and it likely hasn't been trampled yet.
	// hehe... if we start having strange issues here,
	// we should just not clear the slab until next parse when in LUI mode.
	if (!last_token_chart)
		fprintf(stderr, "no chart to display.\n");
	else
		output_lui_chart(last_token_chart);
}

rule_profiler()
{
	int i, j;
	static FILE *f = NULL;
	if (!f)
		f = fopen("/tmp/rule_profile.txt", "w");
	for (i = 0; i < chart_size * chart_size; i++)
	{
		struct chart_cell *c = cells + i;
		for (j = 0; j < c->p_nedges; j++)
		{
			struct edge *e = c->p_edges[j];
			if (!e->rule)
				continue;
			fprintf(f, "%s %d %d %d\n", e->rule->name, e->unpack ? 1 : 0, (e->to - e->from), chart_size - (e->to - e->from));
		}
	}
	fflush(f);
}
